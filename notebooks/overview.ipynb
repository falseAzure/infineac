{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Foal/miniconda3/envs/infineac/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/Foal/miniconda3/envs/infineac/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/Foal/miniconda3/envs/infineac/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/Foal/miniconda3/envs/infineac/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import re\n",
    "import gzip\n",
    "import sys\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from lxml import etree, objectify\n",
    "from rapidfuzz import fuzz\n",
    "from spacy import displacy\n",
    "\n",
    "import infineac.file_loader as file_loader\n",
    "import infineac.helper as helper\n",
    "import infineac.process_event as process_event\n",
    "import infineac.topic_extractor as topic_extractor\n",
    "import infineac.process_text as process_text\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "PATH_DIR = \"../data/transcripts/\"\n",
    "random.seed(111)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all xml files from the given directory and return a list of corresponding events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files: 100%|██████████| 58613/58613 [02:32<00:00, 384.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# files = [Path(\"../data/transcripts/2022/15203138_T.xml\")]\n",
    "files = list(Path(PATH_DIR).rglob(\"*.xml\"))\n",
    "false_part = [files[i] for i in [27246, 27563, 50740, 58498]]\n",
    "events = file_loader.load_files_from_xml(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58613\r"
     ]
    }
   ],
   "source": [
    "positions = []\n",
    "for i, event in enumerate(events):\n",
    "    print(i+1, end=\"\\r\")\n",
    "    if event['qa'] is not None:\n",
    "        for speaker in event[\"qa\"]:\n",
    "            if speaker['position'] not in ['conference', 'cooperation', 'operator', \"unknown participant\"]:\n",
    "                positions.append('' + str(i) + ': ' + speaker['position'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only consider events that are held after 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Events: 100%|██████████| 58613/58613 [00:14<00:00, 4079.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_filt = process_event.filter_events(events, year = 2022, keywords = {})\n",
    "len(events_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = [event['file'] for event in events_filt]\n",
    "id = [event['id'] for event in events_filt]\n",
    "year_upload = [event['year_upload'] for event in events_filt]\n",
    "company = [event['company_name'] for event in events_filt]\n",
    "ticker = [event['company_ticker'] for event in events_filt]\n",
    "ticker_new = [re.sub('\\\\..*', '', t) for t in ticker]\n",
    "dates = [event['date'] for event in events_filt if \"date\" in event.keys()]\n",
    "numeric_values = mdates.date2num(dates)\n",
    "\n",
    "russia_and_sanction = [process_text.get_russia_and_sanction(event['qa_collapsed'] + event['presentation_collapsed']) for event in events_filt]\n",
    "election = [process_text.get_elections(event['qa_collapsed'] + event['presentation_collapsed']) for event in events_filt]\n",
    "russia_count = [str(event['qa_collapsed'] + event['presentation_collapsed']).lower().count('russia') for event in events_filt]\n",
    "sanction_count = [str(event['qa_collapsed'] + event['presentation_collapsed']).lower().count('sanction') for event in events_filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_russia = process_event.filter_events(events, year = 2022, keywords = {'russia': 1, 'ukraine': 1})\n",
    "len(events_russia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy_stanza\n",
    "nlp_stanza = spacy_stanza.load_pipeline(\"en\", processors=\"tokenize\")\n",
    "nlp_stanza.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"\\n\", events_russia[0]['presentation'][3]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_event.extract_parts_from_paragraphs(re.split(\"\\n\", events_russia[0]['presentation'][3]['text']), ['ukraine', \"cgi\"], nlp=nlp_stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_event.extract_parts_from_event(events_russia[0], ['ukraine', 'russia'], nlp=nlp_stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nlp_stanza(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = process_event.extract_parts_from_events(events_russia, {\"russia\": 1, \"ukraine\": 1}, 0, \"list\", \"part\", nlp_stanza)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_russia[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np.array(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(np.array(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy_stanza\n",
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "# nlp = spacy_stanza.load_pipeline(\"en\", processors=\"tokenize, pos, lemma, constituency, depparse, sentiment, ner\")\n",
    "nlp = spacy_stanza.load_pipeline(\"en\", processors=\"tokenize, lemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = process_text.process_corpus(corpus, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_join = [process_text.list_to_string(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "pickle.dump(docs_join, open('../output/pickled/docs_join.pkl', 'wb'))\n",
    "docs_join = pickle.load(open('../output/pickled/docs_join.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model, topics, probs = topic_extractor.bert_inspired(docs_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the topic distributions on a token-level\n",
    "topic_distr, topic_token_distr = topic_model.approximate_distribution(docs_join, calculate_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_distribution(topic_distr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the token-level distributions\n",
    "df = topic_model.visualize_approximate_distribution(docs_join[1], topic_token_distr[1])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "sentence1 = \"Now this slide talks more about the immediate impact where the combination of sanctions and also business destruction in Ukraine leads or may lead to supply chain disruptions.\"\n",
    "sentence2 = \"We obviously intensively evaluate all the potential impacts, created a task force that meets almost daily to follow on these topics and basically do preventive measures as to limit the potential impact on CEZ.\"\n",
    "\n",
    "doc1 = nlp(sentence1)\n",
    "doc2 = nlp(sentence2)\n",
    "similarity = doc1.similarity(doc2)\n",
    "\n",
    "sentences_ = [sentence1, sentence2]\n",
    "sentence_embeddings = model.encode(sentences_)\n",
    "\n",
    "print(\"Cosine Similarity: \" + str(util.pytorch_cos_sim(sentence_embeddings[0], sentence_embeddings[1]).item()))\n",
    "print(\"similiarity: \" + str(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill as pickle\n",
    "# pickle.dump(events_russia, open('../output/pickled/events_russia.pkl', 'wb'))\n",
    "# pickle.dump(events, open('../output/pickled/events.pkl', 'wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame with the following columns and save it as a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'file': file,\n",
    "                   'year_upload': year_upload,\n",
    "                   'company': company,\n",
    "                   'ticker':ticker,\n",
    "                   'ticker_new': ticker_new,\n",
    "                   'date': dates,\n",
    "                   'dates_num' : numeric_values,\n",
    "                   'russia': russia_and_sanction,\n",
    "                   'russia_count': russia_count,\n",
    "                   'sanction_count': sanction_count,\n",
    "                   'election': election})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [event['presentation_collapsed'] + \"\\n\" + event['qa_collapsed'] for event in events_russia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../output/data/overview.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  df[['company', 'ticker', 'ticker_new', 'date', 'russia', 'russia_count', 'sanction_count']].to_csv('../output/data/overview_jakob.csv', index=False)\n",
    "# with open('../output/data/overview_jakob.csv', 'rb') as f_in:\n",
    "#     with gzip.open('../output/data/overview_jakob.csv.gz', 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 15 samples for each category: russia and election and save it in the\n",
    "corresponding directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df[df['russia'] == 'russia & sanctions']) >0:sample_files_russia = df[df['russia'] == 'russia'].sample(8)['file'].tolist() + df[df['russia'] == 'russia & sanctions'].sample(7)['file'].tolist()\n",
    "if len(df[df['election'] == 'presidential election']) >0: sample_files_election = df[df['election'] == 'presidential election'].sample(15)['file'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all files in folder\n",
    "if 'sample_files_russia' in locals() or 'sample_files_russia' in globals():\n",
    "    folder = '../output/sample transcripts/russia/'\n",
    "    files = os.listdir(folder)\n",
    "    for f in files:\n",
    "        os.remove(folder + f)\n",
    "    # copy sample files to folder\n",
    "    for file in sample_files_russia:\n",
    "        shutil.copy(file, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all files in folder\n",
    "if 'sample_files_election' in locals() or sample_files_election in globals():\n",
    "    folder = '../output/sample transcripts/election/'\n",
    "    files = os.listdir(folder)\n",
    "    for f in files:\n",
    "        os.remove(folder + f)\n",
    "    # copy sample files to folder\n",
    "    for file in sample_files_election:\n",
    "        shutil.copy(file, folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of occasions where the word \"russia\" (and \"sanction\") appears in the earnings call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[df['russia_count']>=1], x='russia_count', hue=\"russia\", bins=50, stat=\"percent\", common_norm=True, multiple=\"stack\")\n",
    "plt.xlabel('Count of corresponding terms in the earnings calls')\n",
    "plt.ylabel('Percent')\n",
    "plt.title('Term count')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average mentions of \"russia\" per earnings call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(russia_count).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average mentions of \"russia\" per earnings call if \"russia\" is mentioned at\n",
    "least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([el for el in russia_count if el > 0]).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average mentions of \"sanction\" per earnings call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([event['body_orig'].lower().count('sanction') for event in events]).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event distribution over time highlighting the different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(numeric_values, bins=50)\n",
    "# sns.histplot(numeric_values, bins=50)\n",
    "sns.displot(df, x=\"dates_num\", hue=\"russia\", bins=50, stat=\"percent\", common_norm=True, multiple=\"stack\")\n",
    "# plt.plot_date(dates, numeric_values, '-o')  # '-o' adds markers on the data points\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Event Distribution')\n",
    "\n",
    "# Add x-axis ticks and labels\n",
    "plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # Customize date format as needed\n",
    "\n",
    "plt.gcf().autofmt_xdate()  # Adjusts the date labels rotation for better visibility\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Event proportion over time highlighting the different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(numeric_values, bins=50)\n",
    "# sns.histplot(numeric_values, bins=50)\n",
    "sns.displot(df, x=\"dates_num\", hue=\"russia\", bins=50, stat=\"proportion\", common_norm=True, multiple=\"fill\")\n",
    "# plt.plot_date(dates, numeric_values, '-o')  # '-o' adds markers on the data points\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Propotion')\n",
    "plt.title('Event Propotion')\n",
    "\n",
    "# Add x-axis ticks and labels\n",
    "plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # Customize date format as needed\n",
    "\n",
    "plt.gcf().autofmt_xdate()  # Adjusts the date labels rotation for better visibility\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_keep = ['file', 'body_orig']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_new_trimmed = [{key: event[key] for key in keys_to_keep} for event in events_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsDF = pl.from_dicts(events_new_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventsDF = eventsDF.with_columns(pl.col(\"body_orig\").str.count_match('(E|e)lections').alias(\"elections_count\"))\n",
    "eventsDF = eventsDF.with_columns(pl.col(\"body_orig\").str.count_match('(S|s)anctions').alias(\"sanctions_count\"))\n",
    "eventsDF = eventsDF.with_columns(pl.col(\"body_orig\").str.count_match('(R|r)ussia').alias(\"russia_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eventsDF.filter(pl.col(\"elections_count\") > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eventsDF.filter((pl.col(\"sanctions_count\") > 0) & (pl.col(\"russia_count\") > 0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infineac",
   "language": "python",
   "name": "infineac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
